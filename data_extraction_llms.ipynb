{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Langchain modules\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Error handling https://stackoverflow.com/questions/76958817/streamlit-your-system-has-an-unsupported-version-of-sqlite3-chroma-requires-sq\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "# Import chromadb after updating sqlite3 https://stackoverflow.com/questions/76921252/attributeerror-module-chromadb-has-no-attribute-config\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "# llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 0}, page_content='Self-Supervised Multimodal NeRF for Autonomous Driving\\nGaurav Sharma1, Ravi Kothari 2, Dr. Josef Schmid 3\\nAbstract— In this paper, we propose a Neural Radiance Fields\\n(NeRF) based framework, referred to as Novel View Synthesis\\nFramework (NVSF). It jointly learns the implicit neural rep-\\nresentation of space and time-varying scene for both LiDAR\\nand Camera. We test this on a real-world autonomous driving\\nscenario containing both static and dynamic scenes. Compared\\nto existing multimodal dynamic NeRFs, our framework is self-\\nsupervised, thus eliminating the need for 3D labels. For efficient\\ntraining and faster convergence, we introduce heuristic based\\nimage pixel sampling to focus on pixels with rich information.\\nTo preserve the local features of LiDAR points, a Double\\nGradient based mask is employed. Extensive experiments on\\nthe KITTI-360 dataset show that, compared to the baseline\\nmodels, our Framework has reported best performance on both\\nLiDAR and Camera domain. Code of the model is available at\\nhttps://github.com/gaurav00700/Selfsupervised-NVSF\\nI. INTRODUCTION\\nLight Detection and Ranging (LiDAR) and camera repre-\\nsent two of the most critical sensors in the perception systems\\nof Autonomous Vehicle (A V). However, the collection and\\nannotation of such data is an inherently labor-intensive\\nand costly endeavor, necessitating substantial manual effort\\nand resources. The generation of highly accurate synthetic\\ndata offers a promising alternative, potentially circumventing\\nthese challenges. There are many model-driven approaches\\navailable for generating novel view synthetic LiDAR and\\nCamera data based on scene formulation, such as Game en-\\ngine based [1], [2], Explicit model based [3], [4] and Implicit\\nmodel based [5], [6]. All existing methods come with their\\nown set of limitations. For instance, CARLA [1] operates\\nwithin a handcrafted environment, while some approaches\\nfail to construct view-dependent attributes like raydrop and\\npoint cloud intensity or rely on explicit scene models. As\\na result, these methods struggle to generalise effectively,\\nparticularly in the LiDAR domain, due to the challenges in\\naccurately mapping the physics of active sensors. Achieving\\naccurate and realistic Novel View Synthesis (NVS) is also\\ncrucial for minimizing the sim-to-real domain gap.\\nAdvancements in differential volume rendering and NeRF\\nbased methods have ushered in new possibilities in NVS.\\nThus, we propose the NVSF framework, leveraging NeRF\\nto synthesise novel views for both LiDAR and camera.\\nCentral to the proposed framework is joint learning of\\nimplicit neural representation of scene geometry and view-\\ndependent attributes for both LiDAR and Camera sensors.\\nExploration of LiDAR domain adaptation through the synthe-\\nsis of Point cloud data (PCD) using NeRF spans both static\\nThe author is with the A VL Software and Functions\\nGmbH, Germany and Technische Hochschule Deggen-\\ndorf, Germany gaurav.sharma@avl.com1,\\nravi.kothari@avl.com2, josef.schmid@th-deg.de3\\nFig. 1: Joint Novel View Synthesis for Camera and LiDAR\\nsensor by the proposed method. Along with extrinsic at-\\ntributes, intrinsic sensor parameters can be also varied.\\nand dynamic scenes, with the latter posing unique challenges\\ndue to NeRF’s inherent operating principles. To address\\nthe dynamic scene problem, the proposed framework draws\\ninspiration from LiDAR4D [7] and K-Planes [8], which\\nfactorise a canonical scene, 4D volume of space and time\\nthrough multiple 2D planes. It decomposes a canonical Scene\\nin space and time, thereby making the model interpretable\\nbetween static and dynamic objects. Unlike Neural Scene\\nGraph (NSG) [9] based methods, the proposed framework\\ndoes not require any external supervision from 3D anno-\\ntation to decompose a canonical scene into corresponding\\nforeground and background scenes. To resolve this K-Planes\\nbased network handles dynamic scene of both non-rigid\\nand rigid objects such as pedestrians, cars, trucks, etc. The\\nproposed framework empowers the novel view synthesis\\nof LiDAR and camera in real-world driving scenarios of\\nA Vs, supporting diverse applications such as minimizing\\ndomain gaps, domain invariant training, experimenting with\\ndifferent sensor specifications, sensor configurations on A V\\nand revival of corrupted data.\\nOur contributions are threefold:\\n1) We propose a multimodal self-supervised NVS frame-\\nwork which jointly learns the implicit Neural represen-\\ntation of a 4D spatio-temporal scene for LiDAR and\\nCamera\\n2) We introduce a Heuristic pixel sampling method using\\nMultinomial Probability Distribution based on recon-\\nstruction error of image pixels during training\\n3) To improve feature alignment in synthesised LiDAR\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 1}, page_content='point clouds, we propose a Double Gradient-based\\nclipping mask for capturing sparse features\\nII. RELATED WORK\\n(a) NeRF: NeRF [5] and its derivatives [10]–[12] have\\ndemonstrated remarkable performance in 3D view recon-\\nstruction using multi-view 2D images. Conventional NeRF\\nrelies on Multi Layer Perceptron (MLP) to learn the scene\\ncontent at each queried point, but the need to compute\\nthousands of points along a ray significantly slows down the\\nlearning process. Instant-NGP (iNGP) [13], TensoRF [14]\\nand K-Planes [8] employ learnable embeddings or voxels to\\nrepresent 3D scenes, which significantly reduce the computa-\\ntional burden of the rendering MLP. Recent research efforts\\ncentered on NeRFs have driven significant advancements\\nand notable successes in NVS tasks. A diverse range of\\nneural representations, including MLPs [5], [10]–[12], voxel\\ngrids [15]–[17], tri-planes [18], [19], vector decomposition\\n[14], and multi-level hash grids [13], have been extensively\\nexplored for both reconstruction and synthesis. However,\\nthe majority of these studies are concentrated on object-\\ncentric reconstructions of small indoor scenes. More recently,\\nseveral works [7], [20]–[25] have gradually expanded these\\napproaches to large-scale outdoor environments. However,\\nmost existing approaches focus on either the camera or\\nLiDAR domain, with only a few [26], [27] addressing both\\nmodalities concurrently. A significant challenge remains in\\nhandling both static and dynamic scenes, a crucial require-\\nment for real-world driving scenarios. This limitation impacts\\nthe scalability of such networks. In this paper, we propose\\na unified framework that offers NVS for both LiDAR and\\ncamera.\\n(b) NeRF for LiDAR: Traditional NeRF models were\\ninitially designed for camera images and cannot be directly\\napplied to LiDAR point clouds due to the fundamental\\nstructural differences between these two data types. Unlike\\nimages, which capture dense 2D pixel grids, LiDAR provides\\nsparse, 3D point cloud data, making it challenging to adapt\\nNeRF for LiDAR’s spatial properties. Several efforts have\\nbeen made to extend NeRF to the LiDAR domain. Early\\nnotable works include Neural LiDAR Fields for Novel view\\nsynthesis [28] and LiDAR-NeRF [22], which can synthesise\\nnovel synthesis of LiDAR point cloud. These were followed\\nby additional research, such as NeRF-LiDAR [29], NeuRAD\\n[26] and LiDAR4D [7]. Among these, NeRF-LiDAR [29]\\nand NeuRAD [26] stand out for utilizing both LiDAR and\\ncamera data jointly to learn NeRF fields, enhancing their\\nability to generate more accurate and detailed reconstructions\\nby leveraging the complementary nature of both sensor\\nmodalities. The integration of LiDAR and camera data\\nmitigates the inherent limitations associated with single-\\nmodal approaches, thereby enabling more robust and ef-\\nfective neural radiance field implementations in complex\\n3D environments. However, both of these models utilise\\n3D annotation of LiDAR point cloud for tracking moving\\nobjects. As data annotation is an expensive process therefore,\\nit’s not always possible to invest time and resources in it. In\\nour work, we are learning dynamic scene content in a self-\\nsupervised manner by employing the scene flow module.\\nIII. METHOD\\nThe network of the proposed framework is inspired by\\nexisting works such as NeRF [5], LiDAR-NeRF [22], K-\\nPlanes [8], LiDAR4D [7] and Neural Scene Flow Prior\\n[30]. A novel contribution of the proposed network includes\\nmultimodal training by using both LiDAR and camera data\\nfor jointly learning NeRF models of LiDAR and camera,\\nheuristic image pixel sampling, and improved gradient loss\\nfor better feature alignment. Figure 2 illustrates the proposed\\nnetwork; it comprises of five individual MLPs and two\\nparameterised 4D hybrid encodings using Multi-resolution\\nHash encoding [13] and K-Planes encoding [8]. Each module\\nof the proposed network serves a different learning objective\\nsuch as: (1) Scene Flow MLP learns the forward and\\nbackward scene flow; (2) Positional, Spherical Harmonics\\nand Multi-Resolution Hash encoding encodes positional and\\ntemporal information into high-dimensional feature vectors;\\n(3) 4D Hybrid encoding encodes the space and time infor-\\nmation using multi-resolution hash and 2D-planes for the\\ndecomposition of static and dynamic contents of a scene;\\n(4) NeRF MLP learn the implicit neural representation of\\n4D scene in spatio-temporal coordinate frame; (5) Three\\nradiance MLP handles the view-dependent attributes such as\\nraydrop, intensity of LiDAR point cloud and RGB pixel value\\nof camera image; (6) U-Net is used for global refinement\\nof raydrop for better consistency of synthesised point cloud\\nthroughout the scene.\\nBy combining all modules, there are around 132 mil-\\nlion trainable parameters to be optimised during network\\ntraining. Unlike recent research on NeRF-based multimodal\\nframeworks for NVS such as MARS [31], NeRF-LiDAR\\n[23] and NeuRAD [26], the proposed network decouples\\nstatic (background) and dynamic (foreground) scene com-\\nponents without any external supervision. All modules work\\ncoherently to synthesise views of LiDAR and Camera with\\nhigh fidelity for large-scale static and dynamic scenarios of\\nautonomous vehicle. Therefore, the proposed NVSF of Li-\\nDAR and camera can be formalised as a function of LiDAR\\n(σ, i, p) =flidar(x, θ) and Camera (σ, c) =fcamera(x, θ).\\nWhere i, p, c denotes lidar intensity, ray-drop probabilities\\nand camera RGB values respectively.\\nA. Geometric Feature Alignment\\nLiDAR’s uniform laser projection creates characteristic\\npatterns in point clouds, particularly in flat regions, such\\nas road surface, building walls and car bonnet. The vanilla\\nNeRF struggles with capturing these local features as it’s op-\\ntimised with global pano image objective. As a result, despite\\nperforming well on PCD metrics such as Chamfer distance\\n(CD), it struggles with Peak Signal to noise ratio (PSNR)\\nscore. LiDAR-NeRF [22] proposed structural regularization\\nloss, to improve local geometric consistency.\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 2}, page_content='Fig. 2: The proposed network consists of Scene flow MLP for learning the temporal features, 4D hybrid hash-planes encoding\\nfor the encoding of space and time coordinates, and a shared MLP for learning the scene density and global features. A\\ndifferentiable rendering module for calculating depth, intensity, raydrop of LiDAR point cloud and pixel colour. U-Net\\nnetwork is used for global refinement of LiDAR renderings, as mentioned in LiDAR4D [7].\\nTo compute targeted areas for applying gradient-based\\nloss, single gradients of the ground truth panoramic depth\\nimage were used to create the clipping mask. A threshold\\nwas applied to these gradients to control the coverage of the\\ngradient loss through the mask. However, analysis revealed\\nthat the single gradient-based clipping mask failed to capture\\ndistant regions of the point cloud, as illustrated in Figure 3a.\\nIncreasing the threshold for the clipping mask led to over-\\nsmoothing of the panoramic depth image. As gradient loss\\nwas applied to non-flat areas, reducing the accuracy of the\\nsynthesised point cloud. This issue also affects LiDAR4D\\n[7], which uses the same gradient loss function as evident in\\ntheir codebase.\\nUnlike single gradients, double gradients of pano depth\\nimage of far away regions such as ground and walls of the\\nscene are zero. Gradient loss can be computed on a large\\nrelevant area of point cloud by using a small threshold (eg:\\n0.01) on pano image, as shown in Figure 3b. This method\\ncovers both sparse-texture (far distance) and dense-texture\\n(near distance) regions of the point cloud. Figure 3b shows\\nthe points where Gradient loss is computed. The double\\ngradient mask is better as it can cover more (black region)\\ngeometrically structured regions for feature alignment.\\nGradient loss through a Double gradient-based clipping\\nmask is calculated at small cropped regions (patches) of the\\npano depth image. Gradient loss of both x and y directions\\nare calculated and combined gradient loss for x and y\\ndirections are optimised in local patches. Loss is given by\\nthe Equation 1, where R is the set of training rays of local\\npatches, and GM (·) denotes the gradient operation of x and\\n(a) Single gradient mask\\n (b) Double gradient mask\\nFig. 3: Compared to a single-gradient approach (a), our\\nmethod (b) captures sparse features. The black regions\\nare chosen for structural regularization, while the pink-\\nhighlighted areas are skipped due to their lack of geometric\\nfeatures.\\ny directions. The loss equation is derived from LiDAR-NeRF\\n[22] structural regularization.\\nLgrad =\\n\\x0c\\x0c\\x0c\\n\\x0c\\x0c\\x0c ˆGM (R) − GM (R)\\n\\x0c\\x0c\\x0c\\n\\x0c\\x0c\\x0c\\n1\\n(1)\\nB. Heuristic Ray Sampling\\nNaive NeRF-based models originally used a uniform ray\\nsampling approach, this method assumed that each pixel\\ncontained the same amount of information. However, this\\nassumption fails in large, unbounded scenes, such as those\\nin autonomous driving, where information entropy is non\\nuniform. Failure to consider this leads to NeRF rendering\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 3}, page_content='ghost artifacts, especially around complex objects such as,\\npedestrians. To address this, strategy of Non-uniform sam-\\npling was introduced in [32], but it was targeted for the small\\nindoor 360◦ scenes. The NVSF employs heuristic-based pixel\\nsampling during training, guided by a multinomial probabil-\\nity distribution. This content-aware ray sampling adapts to\\npixel-wise reconstruction loss, giving higher sampling prob-\\nabilities to pixels with higher loss. As illustrated in Figure\\n4, this method draws more pixels with high reconstruction\\nloss compared to random sampling. Since all pixels have a\\npositive likelihood of being sampled, this approach balances\\nexploration and exploitation by focusing on both high-loss\\nand unexplored regions. To introduce variability, sampled\\npixel indices are perturbed through jittering. The framework\\nalternates between heuristic and random sampling across\\ntraining epochs, allowing the network to explore new pixels\\nin one epoch and minimize reconstruction loss in the next,\\nthereby improving focus on challenging pixels.\\nFig. 4: Figure shows the Heuristic based pixel sampling\\nof LiDAR’s Pano image. From top to bottom: Pano depth,\\nReconstruction loss and Heuristic pixel sampling. Red pixels\\nin the middle image indicate high reconstruction loss, thus\\nincreasing their sampling probability, shown as white pixels\\nin the bottom image.\\nC. Multimodal NeRF\\nDepth supervision from LiDAR has already shown to\\nimprove image based NeRF [33], similarly images com-\\nplement the sparse LiDAR data. Building on this NeRF-\\nLiDAR [29] and NeuRAD [26] also use mutlimodal data\\nfor NVS of both sensors. But these networks require 3D\\nannotations for tackling dynamic problem, which involves\\nadditional labelling effort. The proposed framework can\\ndo NVS for both sensors in a self-supervised manner by\\nemploying a 3D flow network. A strategy is required to\\nfuse encoded features effectively for improved convergence\\nof the proposed network. Recent research [34] indicates\\nthat different modalities often contradict each other due\\nto spatial and temporal misalignment. Moreover, LiDAR\\nand camera modalities exhibit inconsistent representations\\nand converge at different rates, leading to uncoordinated\\nconvergence issues [35], [36], [37]. We conduct extensive\\nexperiments as shown in Table I with various combinations\\nof encoded features from both modalities:\\n1) Common encoding for Camera and LiDAR\\n2) Separate encoding for Camera and LiDAR\\n3) Separate encoding for Camera and LiDAR with im-\\nplicit fusion before passing to NeRF MLP\\nSeparate 4D Hybrid Hash-Planar encoding for LiDAR and\\nCamera data performed best among all the three options\\nStrategy\\nLiDAR Camera\\nPCD Range\\nCD ↓ SSIM ↑ PSNR ↑ SSIM ↑ PSNR ↑\\nSame Enc. 0.175 0.76 27.12 0.712 24.97\\nSeparate Enc. 0.110 0.792 27.85 0.811 25.21\\nSeparate Enc.+F. 0.161 0.757 27.52 0.809 25.14\\nTABLE I: Comparison of different encoding strategies.\\nD. Implementation\\nWe follow the classical differential rendering approach\\nused in NeRF models [5], [7], [22]. The view direction\\nof rays is similar to the pattern of a spinning mechanical\\nLiDAR, whose beams are cast in 360◦. The expected depth\\nˆD(r) of a beam can be obtained by integrating over samples\\ngiven by equation 2, which is derived from LiDAR4D [7].\\nˆD(r) =\\nNX\\ni=1\\nTi\\n\\x00\\n1 − e−σiδi\\n\\x01\\nti, α i = 1− e−σiδi (2)\\nWhere Ti = e(−Pi−1\\nj=1 σj δj ) indicates the accumulated\\ntransmittance along ray r to the sampled point ti, and\\nδi = ti+1 − ti is the distance between adjacent samples.\\nti is the depth value of queried points on the ray r, and\\nαi is the opacity value. The expected depth represents the\\ndistance from the LiDAR sensor. Moreover, both the origin\\nand viewing direction of the ray are transformed to the global\\nworld coordinate system, allowing the sampled points ti\\nto be actual points in the real world and consistent across\\nmultiple frames (pano image). The view dependent attributes\\nfor LiDAR are same as in LiDAR4D [7] and for camera\\nattributes are calculated as per Image NeRF [5].\\nAs vehicle motion range may span a long distance in\\nautonomous driving scenarios, it is extremely difficult to\\nestablish long-term correspondences in the canonical space\\nwithout a scene flow network. A few models for the predic-\\ntion of points translation in a space-time volume are Neural\\nscene flow prior [38], Fast Neural Scene Flow (FastNSF)\\n[39], NeuralPCI [40], Dynibar [41] and LiDAR4D [7]. We\\nutilise the LiDAR4D scene flow network to enhance temporal\\nconsistency in spatio-temporal coordinates via a 4D hybrid\\nmulti-resolution hash and plane encoding. This network pre-\\ndicts the prior and post positions of points between adjacent\\nframes and aggregates multi-frame dynamic features for\\ntime-consistent reconstruction. During Flow MLP training,\\nforward and backward scene flow is predicted using the prior\\nand subsequent frames, capturing changes in point positions\\nacross frames. CD is used for calculating geometric loss\\n(Lflow) between the prediction points ˆS and ground truth\\nS points of the current frame as given by equations 3 and\\n4. It encourages the two adjacent frames of the point cloud\\ntransformed by the predicted scene flow to be as consistent\\nas possible.\\nCD = 1\\nK\\nX\\nˆpi∈ˆS\\nmin\\npi∈S\\n||ˆpi−pi||2\\n2+ 1\\nK\\nX\\npi∈S\\nmin\\nˆpi∈ˆS\\n||pi−ˆpi||2\\n2 (3)\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 4}, page_content='Lflow =\\nX\\nj∈±1\\nCD(Si + MLPflow(Si), Si+j), i∈ (0, n−1)\\n(4)\\nTotal reconstruction of the proposed framework is a com-\\nbination of weighted six optimization objectives as shown\\nin the Equation 9. The Lrgb is derived from Image NeRF\\n[5] and the rest of the loss function is a combination of\\nLiDAR4D [7] and LiDAR-NeRF [22].\\nLdepth =\\nX\\nr∈R\\n\\r\\r\\r ˆD(r) − D(r)\\n\\r\\r\\r\\n1\\n(5)\\nLintensity =\\nX\\nr∈R\\n\\r\\r\\rˆI(r) − I(r)\\n\\r\\r\\r\\n2\\n2\\n(6)\\nLraydrop =\\nX\\nr∈R\\n\\r\\r\\r ˆP(r) − P(r)\\n\\r\\r\\r\\n2\\n2\\n(7)\\nLrgb =\\nX\\nr∈R\\n\\r\\r\\r ˆC(r) − C(r)\\n\\r\\r\\r\\n2\\n2\\n(8)\\nLtotal = λ1Lflow + λ2Ldepth(r) +λ3Lintensity(r)\\n+ λ4Lraydrop(r) +λ5Lgrad + λ6Lrgb\\n(9)\\nwhere Ldepth, Lintensity, Lraydrop and Lrgb are losses for\\nDepth, Intensity, Raydrop and RGB of a corresponding pixel.\\nScene flow loss Lflow given by the Equation 4. λ1, λ2, λ3, λ4,\\nλ5 and λ6 are their corresponding weights. Mean Absolute\\nError is used for calculating Depth loss while Mean Square\\nError is used for calculating Intensity loss, Raydrop loss,\\nGradient loss and RGB loss.\\nIV. EXPERIMENTS\\nThe proposed NVSF is implemented based on LiDAR-\\nNeRF [22] and LiDAR4D [7] codebase. The framework is\\ntrained on the KITTI-360 [42] dataset using its Velodyne\\nHDL-64E LiDAR, two front wide-angle cameras and ego\\nposition data. We train on 4 static and 6 dynamic scenes\\n(same as mentioned in LiDAR4D) each comprising 64 indi-\\nvidual frames. Out of this, 60 frames are used for training,\\nand 4 frames are used for validation purposes. The height and\\nwidth of the LiDAR pano image is set to 66 and 1030. Before\\ntraining, LiDAR point clouds are centered and scaled such\\nthat the region of interest falls within a unit cube. All frames\\nare randomly sampled to prevent over-fitting and promote\\nconsistent learning. A ray is sampled uniformly at 768\\npoints in Axis Aligned Bounding Box (AABB) frame. The\\noptimization of NVSF network is implemented in PyTorch\\nwith Adam [43] optimiser. The maximum iterations are set\\nto 30k for each scene, with a batch size of 2048 rays per an\\nimage. Learning rate is 1e-2 with exponential decay during\\niterations and a final decay coefficient of 0.1. The values\\nof weights λ1, λ2, λ3, λ4, λ5 and λ6 are 1.0, 1.0, 0.1, 0.01,\\n0.01 and 1.0 respectively. Remaining parameters such as 4D-\\nHybrid encoding and U-Net are kept same as the LiDAR4D\\nnetwork.\\nA. Baseline\\nFor a thorough evaluation of the NVS framework, a com-\\nprehensive comparison is conducted with LiDAR-NeRF [22]\\nand LiDAR4D [7] models on both static and dynamic scenes\\nreconstruction. LiDAR4D and LiDAR-NeRF has demon-\\nstrated superior performance compared to other LiDAR-\\nbased NVS methods such as LiDARSim [3] and PCGen\\n[44]. Given their proven efficacy, it is deemed unnecessary to\\ninclude these other networks in the comparison. The baseline\\nmodels are trained using their available GitHub code. The\\nprimary focus of this work is to synthesise multimodal data\\nthough unified framework and introduce novel contributions\\nto enhance their performance.\\nModules\\nLiDAR (Foreground)\\nPCD\\nCD ↓\\nRange\\nRMSE ↓\\nIntensity\\nRMSE ↓\\nRaydrop\\nRMSE ↓\\nBaseline 0.220 0.968 0.025 0.093\\n+ Heuristic Sampling 0.173 0.846 0.023 0.080\\n+ Feature alignment 0.158 0.813 0.023 0.079\\n+ Multimodal 0.137 0.833 0.024 0.081\\nNVSF 0.128 0.818 0.023 0.078\\nTABLE II: Comparison of LiDAR based metrics for different\\nmodules. The ablation is conducted over a subset of scenes.\\nB. LiDAR Evaluation\\nWe report evaluation metrics for different attributes such\\nas Point Cloud, Range, Intensity, Raydrop and camera image\\nquality. The metrics are averaged over the 10 scenes, as\\nmentioned above. To analyse dynamic point reconstruction,\\nwe report these metrics for the foreground section too. 3D an-\\nnotation of KITTI360 dataset [42] is used for the foreground\\nseparation. The proposed NVS framework outperformed both\\nLiDAR-NeRF [22] and LiDAR4D [7] models in almost\\nevery PCD synthesis metric. This difference is especially\\nhighlighted in the foreground section, as reported in Table\\nIII. To analyse the effectiveness of individual contributions,\\nwe conduct an ablation study as shown in Table II. Maximum\\nimprovement is shown in the feature alignment, and the\\nefficacy of the multimodal fusion is also evident. Our method\\nhas shown that multimodal NVS can have a shared NeRF\\nwithout impacting individual performance. The framework\\nhas shown improvements of 29% in CD and 12% in Raydrop\\nRoot Mean Square Error (RMSE). Higher values of PSNR\\nsuggest that the synthesised point cloud has less noise\\ncompared to both baseline models.\\nThe proposed network has accurately synthesised back-\\nground and foreground objects with view-dependent at-\\ntributes compared to baseline and LiDAR4D [7] networks, as\\nshown in Figure 5. Its PCD has better feature representation,\\nconsistency and high fidelity.\\nC. Camera Evaluation\\nThe proposed NVS framework uses multimodal data from\\nLiDAR and camera for training the network, thereby jointly\\nlearning implicit Neural scene representation and view-\\ndependent attributes. The framework can synthesise camera\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 5}, page_content='Method\\nLiDAR\\nPCD Range Intensity Raydrop\\nCD ↓ F-score ↑ RMSE ↓ LPIPS ↓ SSIM ↑ PSNR ↑ RMSE ↓ LPIPS ↓ SSIM ↑ PSNR ↑ RMSE ↓ Acc.↑ F-score ↑\\nStatic + Dynamic Scene\\nLiDAR-Nerf [22] 0.114 0.918 3.982 0.309 0.623 26.208 0.152 0.363 0.325 16.442 0.356 0.832 0.882\\nLiDAR4D [7] 0.129 0.905 3.240 0.108 0.796 28.045 0.117 0.193 0.564 18.667 0.274 0.921 0.937\\nNVSF 0.091 0.929 3.150 0.091 0.819 28.322 0.115 0.163 0.579 18.862 0.240 0.925 0.947\\nDynamic Scene (Foreground)\\nLiDAR-Nerf [22] 0.758 0.408 0.799 0.053 0.967 40.885 0.025 0.033 0.973 32.941 0.090 0.988 0.411\\nLiDAR4D [7] 0.255 0.501 0.726 0.039 0.973 41.436 0.021 0.026 0.979 34.314 0.076 0.991 0.619\\nNVSF 0.111 0.720 0.645 0.030 0.976 42.459 0.020 0.019 0.979 34.870 0.069 0.992 0.733\\nTABLE III: Quantitative comparison on KITTI 360 Dataset. We report LiDAR based metrics for Static, Dynamic and\\nForeground (dynamic lidar Points) scenes. Bold and underlined values are best and second best respectively for the\\ncorresponding metrics.\\nFig. 5: Comparison of error map of synthesised PCD by\\nLiDAR-NeRF, LiDAR4D and Proposed NVSF for dynamic\\nsequence 4950. Red colour points are indicating an error of\\nmore than 1.0m with respect to ground truth.\\nimages along with LiDAR point clouds. Since neither of\\nthe Baseline networks support joint synthesis of LiDAR and\\nCamera data, we only report evaluation of our work. Table\\nIV shows the image quality metrics for NVSF model. The\\ncombined PSNR value is low when compared to foreground\\nsection, this could be due to the presence of limited dynamic\\nobjects. To give a perspective on our quality of image\\nsynthesis, NeuRad [26] (supervised approach) has reported\\na PSNR value of 27 for KITTI MOT dataset [45]. This\\nindicates the effectiveness of self-supervised approach for\\ndynamic object view synthesis.\\nScenes Camera\\nRMSE ↓ LPIPS ↓ SSIM ↑ PSNR ↑\\nStatic + Dynamic 1.305 0.217 0.817 25.432\\nDynamic (Foreground) 0.344 0.017 0.736 49.057\\nTABLE IV: Table shows the evaluation result for the recon-\\nstruction of the Camera image for KITTI 360 Dataset.\\nV. C ONCLUSION\\nThis paper has proposed a NVS framework for synthesis\\nof both LiDAR and Camera data by learning implicit Neural\\nrepresentation of a 4D scene and view-dependent attributes\\nin a canonical frame. We report a best performance on quan-\\ntitative and qualitative metrics when compared to baselines,\\nLiDAR-NeRF and LiDAR4D. We revisit the fundamentals of\\nray sampling and improve upon this by introducing Heuristic\\nsampling and Double gradient mask. The proposed network\\nis able to synthesise both static and dynamic scenes in a\\nself-supervised manner. Despite the performance, the current\\nself-supervision approach for dynamic scene reconstruction\\nbinds foreground and background scenes, which restrict\\nNVS within the ego-vehicle trajectory in the spatio-temporal\\ncoordinate space. Consequently, scene editing or scene ma-\\nnipulation is not possible outside ego-vehicle trajectory. We\\nforesee the future multimodal work will focus on improving\\nthe reconstruction quality over larger scenes without 3D\\nannotation supervision.\\nVI. A CKNOWLEDGEMENT\\nThis work is a part of a master thesis supported by A VL\\nSoftware and Functions GmbH, Germany and Technische\\nHochschule Deggendorf, Germany. We are thankful to pub-\\nlishers of LiDAR-NeRF and LiDAR4D papers for their great\\nwork and making the codebase publicly available for further\\nresearch.\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 6}, page_content='REFERENCES\\n[1] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V . Koltun,\\n“CARLA: An open urban driving simulator,” in Conference on robot\\nlearning. PMLR, 2017, pp. 1–16.\\n[2] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “AirSim: High-Fidelity\\nVisual and Physical Simulation for Autonomous Vehicles,” in Field\\nand Service Robotics: Results of the 11th International Conference .\\nSpringer, 2018, pp. 621–635.\\n[3] S. Manivasagam, S. Wang, K. Wong, W. Zeng, M. Sazanovich,\\nS. Tan, B. Yang, W.-C. Ma, and R. Urtasun, “LiDARsim: Realistic\\nLiDAR simulation by leveraging the real world,” in Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\\n2020, pp. 11 167–11 176.\\n[4] M. Gschwandtner, R. Kwitt, A. Uhl, and W. Pree, “BlenSor: Blender\\nsensor simulation toolbox,” in Advances in Visual Computing: 7th\\nInternational Symposium, ISVC 2011, Las Vegas, NV , USA, September\\n26-28, 2011. Proceedings, Part II 7 . Springer, 2011, pp. 199–208.\\n[5] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoor-\\nthi, and R. Ng, “NeRF: Representing scenes as neural radiance fields\\nfor view synthesis,” Communications of the ACM , vol. 65, no. 1, pp.\\n99–106, 2021.\\n[6] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\\nS. Ozair, A. Courville, and Y . Bengio, “Generative Adversarial Net-\\nworks,” Communications of the ACM , vol. 63, no. 11, pp. 139–144,\\n2020.\\n[7] Z. Zheng, F. Lu, W. Xue, G. Chen, and C. Jiang, “LiDAR4D: Dynamic\\nNeural Fields for Novel Space-time View LiDAR Synthesis,” arXiv\\npreprint arXiv:2404.02742, 2024.\\n[8] S. Fridovich-Keil, G. Meanti, F. R. Warburg, B. Recht, and\\nA. Kanazawa, “K-planes: Explicit radiance fields in space, time,\\nand appearance,” in Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , 2023, pp. 12 479–12 488.\\n[9] J. Ost, F. Mannan, N. Thuerey, J. Knodt, and F. Heide, “Neural\\nScene Graphs for Dynamic Scenes,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2021, pp.\\n2856–2865.\\n[10] J. T. Barron, B. Mildenhall, M. Tancik, P. Hedman, R. Martin-Brualla,\\nand P. P. Srinivasan, “Mip-NeRF: A multiscale representation for\\nanti-aliasing neural radiance fields,” in Proceedings of the IEEE/CVF\\nInternational Conference on Computer Vision , 2021, pp. 5855–5864.\\n[11] J. T. Barron, B. Mildenhall, D. Verbin, P. P. Srinivasan, and P. Hedman,\\n“Mip-NeRF 360: Unbounded anti-aliased neural radiance fields,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2022, pp. 5470–5479.\\n[12] ——, “Zip-NeRF: Anti-aliased grid-based neural radiance fields,” in\\nProceedings of the IEEE/CVF International Conference on Computer\\nVision, 2023, pp. 19 697–19 705.\\n[13] T. M ¨uller, A. Evans, C. Schied, and A. Keller, “Instant neural graphics\\nprimitives with a multiresolution hash encoding,” ACM transactions\\non graphics (TOG) , vol. 41, no. 4, pp. 1–15, 2022.\\n[14] A. Chen, Z. Xu, A. Geiger, J. Yu, and H. Su, “Tensorf: Tensorial ra-\\ndiance fields,” in European conference on computer vision. Springer,\\n2022, pp. 333–350.\\n[15] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and\\nA. Kanazawa, “Plenoxels: Radiance fields without neural networks,”\\nin Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition, 2022, pp. 5501–5510.\\n[16] L. Liu, J. Gu, K. Zaw Lin, T.-S. Chua, and C. Theobalt, “Neural sparse\\nvoxel fields,” Advances in Neural Information Processing Systems ,\\nvol. 33, pp. 15 651–15 663, 2020.\\n[17] C. Sun, M. Sun, and H.-T. Chen, “Direct voxel grid optimization:\\nSuper-fast convergence for radiance fields reconstruction,” in Proceed-\\nings of the IEEE/CVF conference on computer vision and pattern\\nrecognition, 2022, pp. 5459–5469.\\n[18] E. R. Chan, C. Z. Lin, M. A. Chan, K. Nagano, B. Pan, S. De Mello,\\nO. Gallo, L. J. Guibas, J. Tremblay, S. Khamis, et al. , “Efficient\\ngeometry-aware 3d generative adversarial networks,” inProceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition,\\n2022, pp. 16 123–16 133.\\n[19] W. Hu, Y . Wang, L. Ma, B. Yang, L. Gao, X. Liu, and Y . Ma, “Tri-\\nMipRF: Tri-Mip representation for efficient anti-aliasing neural radi-\\nance fields,” in Proceedings of the IEEE/CVF International Conference\\non Computer Vision , 2023, pp. 19 774–19 783.\\n[20] S. Sun, B. Zhuang, Z. Jiang, B. Liu, X. Xie, and M. Chandraker,\\n“LidaRF: Delving into LiDAR for neural radiance field on street\\nscenes,” arXiv preprint arXiv:2405.00900 , 2024.\\n[21] M. Tancik, V . Casser, X. Yan, S. Pradhan, B. Mildenhall, P. P.\\nSrinivasan, J. T. Barron, and H. Kretzschmar, “Block-NeRF: Scalable\\nlarge scene neural view synthesis,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2022, pp.\\n8248–8258.\\n[22] T. Tao, L. Gao, G. Wang, Y . Lao, P. Chen, H. Zhao, D. Hao, X. Liang,\\nM. Salzmann, and K. Yu, “LiDAR-NeRF: Novel LiDAR view synthe-\\nsis via neural radiance fields,” arXiv preprint arXiv:2304.10406, 2023.\\n[23] J. Zhang, F. Zhang, S. Kuang, and L. Zhang, “NeRF-LiDAR: Gener-\\nating Realistic LiDAR Point Clouds with Neural Radiance Fields,” in\\nAAAI Conference on Artificial Intelligence (AAAI) , 2024.\\n[24] K. Rematas, A. Liu, P. P. Srinivasan, J. T. Barron, A. Tagliasacchi,\\nT. Funkhouser, and V . Ferrari, “Urban radiance fields,” in Proceed-\\nings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition, 2022, pp. 12 932–12 942.\\n[25] Z. Wang, T. Shen, J. Gao, S. Huang, J. Munkberg, J. Hasselgren,\\nZ. Gojcic, W. Chen, and S. Fidler, “Neural fields meet explicit\\ngeometric representations for inverse rendering of urban scenes,” in\\nProceedings of the IEEE/CVF Conference on Computer Vision and\\nPattern Recognition, 2023, pp. 8370–8380.\\n[26] A. Tonderski, C. Lindstr ¨om, G. Hess, W. Ljungbergh, L. Svensson, and\\nC. Petersson, “NeuRAD: Neural rendering for autonomous driving,”\\narXiv preprint arXiv:2311.15260 , 2023.\\n[27] Z. Yang, Y . Chen, J. Wang, S. Manivasagam, W.-C. Ma, A. J. Yang,\\nand R. Urtasun, “UniSim: A Neural Closed-Loop Sensor Simulator,”\\nin CVPR, 2023.\\n[28] S. Huang, Z. Gojcic, Z. Wang, F. Williams, Y . Kasten, S. Fidler,\\nK. Schindler, and O. Litany, “Neural LiDAR fields for novel view\\nsynthesis,” in Proceedings of the IEEE/CVF International Conference\\non Computer Vision , 2023, pp. 18 236–18 246.\\n[29] J. Zhang, F. Zhang, S. Kuang, and L. Zhang, “NeRF-LiDAR: Gen-\\nerating realistic LiDAR point clouds with neural radiance fields,” in\\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 38,\\nno. 7, 2024, pp. 7178–7186.\\n[30] X. Li, J. Kaesemodel Pontes, and S. Lucey, “Neural Scene Flow\\nPrior,” in Advances in Neural Information Processing Systems ,\\nM. Ranzato, A. Beygelzimer, Y . Dauphin, P. Liang, and J. W.\\nVaughan, Eds., vol. 34. Curran Associates, Inc., 2021, pp. 7838–\\n7851. [Online]. Available: https://proceedings.neurips.cc/paper files/\\npaper/2021/file/41263b9a46f6f8f22668476661614478-Paper.pdf\\n[31] Z. Wu, T. Liu, L. Luo, Z. Zhong, J. Chen, H. Xiao, C. Hou, H. Lou,\\nY . Chen, R. Yang, et al. , “MARS: An instance-aware, modular and\\nrealistic simulator for autonomous driving,” in CAAI International\\nConference on Artificial Intelligence . Springer, 2023, pp. 3–15.\\n[32] T. Otonari, S. Ikehata, and K. Aizawa, “Non-uniform sampling strate-\\ngies for NeRF on 360° images.” in BMVC, 2022, p. 344.\\n[33] K. Deng, A. Liu, J.-Y . Zhu, and D. Ramanan, “Depth-supervised\\nNeRF: Fewer Views and Faster Training for Free,” in 2022 IEEE/CVF\\nConference on Computer Vision and Pattern Recognition (CVPR) ,\\n2022, pp. 12 872–12 881.\\n[34] T. Tang, G. Wang, Y . Lao, P. Chen, J. Liu, L. Lin, K. Yu, and X. Liang,\\n“AlignMiF: Geometry-Aligned Multimodal Implicit Field for LiDAR-\\nCamera Joint Synthesis,” arXiv preprint arXiv:2402.17483 , 2024.\\n[35] X. Peng, Y . Wei, A. Deng, D. Wang, and D. Hu, “Balanced multimodal\\nlearning via on-the-fly gradient modulation,” in Proceedings of the\\nIEEE/CVF conference on computer vision and pattern recognition ,\\n2022, pp. 8238–8247.\\n[36] Y . Sun, S. Mai, and H. Hu, “Learning to balance the learning rates\\nbetween various modalities via adaptive tracking factor,” IEEE Signal\\nProcessing Letters, vol. 28, pp. 1650–1654, 2021.\\n[37] W. Wang, D. Tran, and M. Feiszli, “What makes training multi-\\nmodal classification networks hard?” in Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition , 2020, pp.\\n12 695–12 705.\\n[38] X. Li, J. Kaesemodel Pontes, and S. Lucey, “Neural scene flow prior,”\\nAdvances in Neural Information Processing Systems , vol. 34, pp.\\n7838–7851, 2021.\\n[39] X. Li, J. Zheng, F. Ferroni, J. K. Pontes, and S. Lucey, “Fast\\nNeural Scene Flow,” in Proceedings of the IEEE/CVF International\\nConference on Computer Vision (ICCV) , October 2023, pp. 9878–\\n9890.\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'source': 'data/ICRA25_0661_MS.pdf', 'page': 7}, page_content='[40] Z. Zheng, D. Wu, R. Lu, F. Lu, G. Chen, and C. Jiang, “NeuralPCI:\\nSpatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-\\nlinear Interpolation,” in Proceedings of the IEEE/CVF Conference on\\nComputer Vision and Pattern Recognition , 2023, pp. 909–918.\\n[41] Z. Li, Q. Wang, F. Cole, R. Tucker, and N. Snavely, “DynIBaR: Neural\\ndynamic image-based rendering,” in Proceedings of the IEEE/CVF\\nConference on Computer Vision and Pattern Recognition , 2023, pp.\\n4273–4284.\\n[42] Y . Liao, J. Xie, and A. Geiger, “KITTI-360: A novel dataset and\\nbenchmarks for urban scene understanding in 2d and 3d,” IEEE\\nTransactions on Pattern Analysis and Machine Intelligence , vol. 45,\\nno. 3, pp. 3292–3310, 2022.\\n[43] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimiza-\\ntion,” arXiv preprint arXiv:1412.6980 , 2014.\\n[44] G. A. Allen and L. M. Baldwin, “PCGen: A FORTRAN IV program\\nto generate paired-comparison stimuli,” Behavior Research Methods\\n& Instrumentation, vol. 12, no. 3, pp. 383–384, 1980.\\n[45] P. V oigtlaender, M. Krause, A. Osep, J. Luiten, B. B. G. Sekar,\\nA. Geiger, and B. Leibe, “MOTS: Multi-Object Tracking and Segmen-\\ntation,” in Conference on Computer Vision and Pattern Recognition\\n(CVPR), 2019.\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"data/ICRA25_0661_MS.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Self-Supervised Multimodal NeRF for Autonomous Driving\\nGaurav Sharma1, Ravi Kothari 2, Dr. Josef Schmid 3\\nAbstract— In this paper, we propose a Neural Radiance Fields\\n(NeRF) based framework, referred to as Novel View Synthesis\\nFramework (NVSF). It jointly learns the implicit neural rep-\\nresentation of space and time-varying scene for both LiDAR\\nand Camera. We test this on a real-world autonomous driving\\nscenario containing both static and dynamic scenes. Compared\\nto existing multimodal dynamic NeRFs, our framework is self-\\nsupervised, thus eliminating the need for 3D labels. For efficient\\ntraining and faster convergence, we introduce heuristic based\\nimage pixel sampling to focus on pixels with rich information.\\nTo preserve the local features of LiDAR points, a Double\\nGradient based mask is employed. Extensive experiments on\\nthe KITTI-360 dataset show that, compared to the baseline\\nmodels, our Framework has reported best performance on both\\nLiDAR and Camera domain. Code of the model is available at\\nhttps://github.com/gaurav00700/Selfsupervised-NVSF\\nI. INTRODUCTION\\nLight Detection and Ranging (LiDAR) and camera repre-\\nsent two of the most critical sensors in the perception systems\\nof Autonomous Vehicle (A V). However, the collection and\\nannotation of such data is an inherently labor-intensive\\nand costly endeavor, necessitating substantial manual effort\\nand resources. The generation of highly accurate synthetic\\ndata offers a promising alternative, potentially circumventing'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "# test_vector = embedding_function.embed_query(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "                            embeddings=embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Paris\", reference=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk)\n",
    "        else:\n",
    "            print('Dropping duplicate chunk')\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                 embedding_function=embedding_function, \n",
    "                                 vectorstore_path=\"vectorstore_nerf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query for relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorstore\n",
    "vectorstore = Chroma(persist_directory=\"vectorstore_nerf\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 0, 'source': 'data/ICRA25_0661_MS.pdf'}, page_content='and rigid objects such as pedestrians, cars, trucks, etc. The\\nproposed framework empowers the novel view synthesis\\nof LiDAR and camera in real-world driving scenarios of\\nA Vs, supporting diverse applications such as minimizing\\ndomain gaps, domain invariant training, experimenting with\\ndifferent sensor specifications, sensor configurations on A V\\nand revival of corrupted data.\\nOur contributions are threefold:\\n1) We propose a multimodal self-supervised NVS frame-\\nwork which jointly learns the implicit Neural represen-\\ntation of a 4D spatio-temporal scene for LiDAR and\\nCamera\\n2) We introduce a Heuristic pixel sampling method using\\nMultinomial Probability Distribution based on recon-\\nstruction error of image pixels during training\\n3) To improve feature alignment in synthesised LiDAR\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'page': 4, 'source': 'data/ICRA25_0661_MS.pdf'}, page_content='III. To analyse the effectiveness of individual contributions,\\nwe conduct an ablation study as shown in Table II. Maximum\\nimprovement is shown in the feature alignment, and the\\nefficacy of the multimodal fusion is also evident. Our method\\nhas shown that multimodal NVS can have a shared NeRF\\nwithout impacting individual performance. The framework\\nhas shown improvements of 29% in CD and 12% in Raydrop\\nRoot Mean Square Error (RMSE). Higher values of PSNR\\nsuggest that the synthesised point cloud has less noise\\ncompared to both baseline models.\\nThe proposed network has accurately synthesised back-\\nground and foreground objects with view-dependent at-\\ntributes compared to baseline and LiDAR4D [7] networks, as\\nshown in Figure 5. Its PCD has better feature representation,\\nconsistency and high fidelity.\\nC. Camera Evaluation\\nThe proposed NVS framework uses multimodal data from\\nLiDAR and camera for training the network, thereby jointly\\nlearning implicit Neural scene representation and view-\\ndependent attributes. The framework can synthesise camera\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'page': 2, 'source': 'data/ICRA25_0661_MS.pdf'}, page_content='1\\n(1)\\nB. Heuristic Ray Sampling\\nNaive NeRF-based models originally used a uniform ray\\nsampling approach, this method assumed that each pixel\\ncontained the same amount of information. However, this\\nassumption fails in large, unbounded scenes, such as those\\nin autonomous driving, where information entropy is non\\nuniform. Failure to consider this leads to NeRF rendering\\nCONFIDENTIAL. Limited circulation. For review only.\\nManuscript 661 submitted to 2025 IEEE International Conference on\\nRobotics and Automation (ICRA). Received September 16, 2024.'),\n",
       " Document(metadata={'page': 0, 'source': 'data/ICRA25_0661_MS.pdf'}, page_content='and costly endeavor, necessitating substantial manual effort\\nand resources. The generation of highly accurate synthetic\\ndata offers a promising alternative, potentially circumventing\\nthese challenges. There are many model-driven approaches\\navailable for generating novel view synthetic LiDAR and\\nCamera data based on scene formulation, such as Game en-\\ngine based [1], [2], Explicit model based [3], [4] and Implicit\\nmodel based [5], [6]. All existing methods come with their\\nown set of limitations. For instance, CARLA [1] operates\\nwithin a handcrafted environment, while some approaches\\nfail to construct view-dependent attributes like raydrop and\\npoint cloud intensity or rely on explicit scene models. As\\na result, these methods struggle to generalise effectively,\\nparticularly in the LiDAR domain, due to the challenges in\\naccurately mapping the physics of active sensors. Achieving\\naccurate and realistic Novel View Synthesis (NVS) is also\\ncrucial for minimizing the sim-to-real domain gap.\\nAdvancements in differential volume rendering and NeRF\\nbased methods have ushered in new possibilities in NVS.\\nThus, we propose the NVSF framework, leveraging NeRF\\nto synthesise novel views for both LiDAR and camera.\\nCentral to the proposed framework is joint learning of\\nimplicit neural representation of scene geometry and view-\\ndependent attributes for both LiDAR and Camera sensors.\\nExploration of LiDAR domain adaptation through the synthe-')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the novelty of the paper?\")\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: \n",
      "You are an assistant for question-answering tasks.\n",
      "Use the following pieces of retrieved context to answer\n",
      "the question. If you don't know the answer, say that you\n",
      "don't know. DON'T MAKE UP ANYTHING.\n",
      "\n",
      "and rigid objects such as pedestrians, cars, trucks, etc. The\n",
      "proposed framework empowers the novel view synthesis\n",
      "of LiDAR and camera in real-world driving scenarios of\n",
      "A Vs, supporting diverse applications such as minimizing\n",
      "domain gaps, domain invariant training, experimenting with\n",
      "different sensor specifications, sensor configurations on A V\n",
      "and revival of corrupted data.\n",
      "Our contributions are threefold:\n",
      "1) We propose a multimodal self-supervised NVS frame-\n",
      "work which jointly learns the implicit Neural represen-\n",
      "tation of a 4D spatio-temporal scene for LiDAR and\n",
      "Camera\n",
      "2) We introduce a Heuristic pixel sampling method using\n",
      "Multinomial Probability Distribution based on recon-\n",
      "struction error of image pixels during training\n",
      "3) To improve feature alignment in synthesised LiDAR\n",
      "CONFIDENTIAL. Limited circulation. For review only.\n",
      "Manuscript 661 submitted to 2025 IEEE International Conference on\n",
      "Robotics and Automation (ICRA). Received September 16, 2024.\n",
      "\n",
      "---\n",
      "\n",
      "III. To analyse the effectiveness of individual contributions,\n",
      "we conduct an ablation study as shown in Table II. Maximum\n",
      "improvement is shown in the feature alignment, and the\n",
      "efficacy of the multimodal fusion is also evident. Our method\n",
      "has shown that multimodal NVS can have a shared NeRF\n",
      "without impacting individual performance. The framework\n",
      "has shown improvements of 29% in CD and 12% in Raydrop\n",
      "Root Mean Square Error (RMSE). Higher values of PSNR\n",
      "suggest that the synthesised point cloud has less noise\n",
      "compared to both baseline models.\n",
      "The proposed network has accurately synthesised back-\n",
      "ground and foreground objects with view-dependent at-\n",
      "tributes compared to baseline and LiDAR4D [7] networks, as\n",
      "shown in Figure 5. Its PCD has better feature representation,\n",
      "consistency and high fidelity.\n",
      "C. Camera Evaluation\n",
      "The proposed NVS framework uses multimodal data from\n",
      "LiDAR and camera for training the network, thereby jointly\n",
      "learning implicit Neural scene representation and view-\n",
      "dependent attributes. The framework can synthesise camera\n",
      "CONFIDENTIAL. Limited circulation. For review only.\n",
      "Manuscript 661 submitted to 2025 IEEE International Conference on\n",
      "Robotics and Automation (ICRA). Received September 16, 2024.\n",
      "\n",
      "---\n",
      "\n",
      "1\n",
      "(1)\n",
      "B. Heuristic Ray Sampling\n",
      "Naive NeRF-based models originally used a uniform ray\n",
      "sampling approach, this method assumed that each pixel\n",
      "contained the same amount of information. However, this\n",
      "assumption fails in large, unbounded scenes, such as those\n",
      "in autonomous driving, where information entropy is non\n",
      "uniform. Failure to consider this leads to NeRF rendering\n",
      "CONFIDENTIAL. Limited circulation. For review only.\n",
      "Manuscript 661 submitted to 2025 IEEE International Conference on\n",
      "Robotics and Automation (ICRA). Received September 16, 2024.\n",
      "\n",
      "---\n",
      "\n",
      "and costly endeavor, necessitating substantial manual effort\n",
      "and resources. The generation of highly accurate synthetic\n",
      "data offers a promising alternative, potentially circumventing\n",
      "these challenges. There are many model-driven approaches\n",
      "available for generating novel view synthetic LiDAR and\n",
      "Camera data based on scene formulation, such as Game en-\n",
      "gine based [1], [2], Explicit model based [3], [4] and Implicit\n",
      "model based [5], [6]. All existing methods come with their\n",
      "own set of limitations. For instance, CARLA [1] operates\n",
      "within a handcrafted environment, while some approaches\n",
      "fail to construct view-dependent attributes like raydrop and\n",
      "point cloud intensity or rely on explicit scene models. As\n",
      "a result, these methods struggle to generalise effectively,\n",
      "particularly in the LiDAR domain, due to the challenges in\n",
      "accurately mapping the physics of active sensors. Achieving\n",
      "accurate and realistic Novel View Synthesis (NVS) is also\n",
      "crucial for minimizing the sim-to-real domain gap.\n",
      "Advancements in differential volume rendering and NeRF\n",
      "based methods have ushered in new possibilities in NVS.\n",
      "Thus, we propose the NVSF framework, leveraging NeRF\n",
      "to synthesise novel views for both LiDAR and camera.\n",
      "Central to the proposed framework is joint learning of\n",
      "implicit neural representation of scene geometry and view-\n",
      "dependent attributes for both LiDAR and Camera sensors.\n",
      "Exploration of LiDAR domain adaptation through the synthe-\n",
      "\n",
      "---\n",
      "\n",
      "Answer the question based on the above context: What is the novelty of the paper?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the novelty of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The novelty of the paper lies in the proposed multimodal self-supervised Novel View Synthesis (NVS) framework, which jointly learns the implicit neural representation of a 4D spatio-temporal scene for both LiDAR and camera data. The key contributions include:\\n\\n1. A multimodal self-supervised NVS framework that integrates data from LiDAR and camera to enhance scene representation.\\n2. A heuristic pixel sampling method based on reconstruction error to improve training efficiency.\\n3. Improved feature alignment in synthesized LiDAR data, demonstrating that multimodal NVS can effectively share a Neural Radiance Field (NeRF) without compromising individual performance.\\n\\nThe framework shows significant improvements in feature representation, consistency, and fidelity of synthesized point clouds compared to baseline models, addressing the challenges of accurately mapping the physics of active sensors and minimizing the sim-to-real domain gap.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 170, 'prompt_tokens': 966, 'total_tokens': 1136, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-7c59f615-2240-4f12-b7a6-94c3c0c6102a-0', usage_metadata={'input_tokens': 966, 'output_tokens': 170, 'total_tokens': 1136})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The title of the paper is not provided in the retrieved context. Therefore, I don't know the title of the paper.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 1220, 'total_tokens': 1244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0705bf87c0', 'finish_reason': 'stop', 'logprobs': None}, id='run-b137c24a-5a54-4b47-95bd-fd20fe5c92d9-0', usage_metadata={'input_tokens': 1220, 'output_tokens': 24, 'total_tokens': 1244})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate structured responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtractedInfo(paper_title=AnswerWithSources(answer='Multimodal Self-Supervised Neural View Synthesis Framework for LiDAR and Camera', sources='The proposed framework empowers the novel view synthesis of LiDAR and camera in real-world driving scenarios of A Vs.', reasoning=\"The title is derived from the context that describes the framework's capabilities in synthesizing views from LiDAR and camera.\"), paper_summary=AnswerWithSources(answer='The paper proposes a multimodal self-supervised neural view synthesis (NVS) framework that jointly learns the implicit neural representation of a 4D spatio-temporal scene for LiDAR and camera data. It introduces a heuristic pixel sampling method based on reconstruction error and improves feature alignment in synthesized LiDAR data, demonstrating significant improvements in performance metrics.', sources='Our contributions are threefold: 1) We propose a multimodal self-supervised NVS framework which jointly learns the implicit Neural representation of a 4D spatio-temporal scene for LiDAR and Camera... The proposed network has accurately synthesised background and foreground objects with view-dependent attributes compared to baseline and LiDAR4D networks.', reasoning='The summary encapsulates the key contributions and findings of the paper based on the detailed context provided.'), publication_year=AnswerWithSources(answer='2024', sources='Received September 16, 2024.', reasoning='The publication year is indicated by the date the manuscript was received.'), paper_authors=AnswerWithSources(answer='X. Li, J. Kaesemodel Pontes, and S. Lucey', sources='[30] X. Li, J. Kaesemodel Pontes, and S. Lucey, “Neural Scene Flow Prior,” in Advances in Neural Information Processing Systems.', reasoning='The authors of the paper are directly mentioned in the context.'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo, strict=True)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform response into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>paper_summary</th>\n",
       "      <th>publication_year</th>\n",
       "      <th>paper_authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Multimodal Self-Supervised Neural View Synthes...</td>\n",
       "      <td>The paper proposes a multimodal self-supervise...</td>\n",
       "      <td>2025</td>\n",
       "      <td>X. Li, J. Kaesemodel Pontes, and S. Lucey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <td>The proposed framework empowers the novel view...</td>\n",
       "      <td>Our contributions are threefold: 1) We propose...</td>\n",
       "      <td>Manuscript 661 submitted to 2025 IEEE Internat...</td>\n",
       "      <td>[30] X. Li, J. Kaesemodel Pontes, and S. Lucey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reasoning</th>\n",
       "      <td>The title is inferred from the context discuss...</td>\n",
       "      <td>The summary encapsulates the main contribution...</td>\n",
       "      <td>The publication year is stated directly in the...</td>\n",
       "      <td>The authors are mentioned in the context relat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 paper_title  \\\n",
       "answer     Multimodal Self-Supervised Neural View Synthes...   \n",
       "source     The proposed framework empowers the novel view...   \n",
       "reasoning  The title is inferred from the context discuss...   \n",
       "\n",
       "                                               paper_summary  \\\n",
       "answer     The paper proposes a multimodal self-supervise...   \n",
       "source     Our contributions are threefold: 1) We propose...   \n",
       "reasoning  The summary encapsulates the main contribution...   \n",
       "\n",
       "                                            publication_year  \\\n",
       "answer                                                  2025   \n",
       "source     Manuscript 661 submitted to 2025 IEEE Internat...   \n",
       "reasoning  The publication year is stated directly in the...   \n",
       "\n",
       "                                               paper_authors  \n",
       "answer             X. Li, J. Kaesemodel Pontes, and S. Lucey  \n",
       "source     [30] X. Li, J. Kaesemodel Pontes, and S. Lucey...  \n",
       "reasoning  The authors are mentioned in the context relat...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
