{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import streamlit as st  \n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Import Langchain modules\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Error handling https://stackoverflow.com/questions/76958817/streamlit-your-system-has-an-unsupported-version-of-sqlite3-chroma-requires-sq\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "\n",
    "# Import chromadb after updating sqlite3 https://stackoverflow.com/questions/76921252/attributeerror-module-chromadb-has-no-attribute-config\n",
    "from langchain.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=OPENAI_API_KEY)\n",
    "llm.invoke(\"Tell me a joke about cats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"data/Oppenheimer-2006-Applied_Cognitive_Psychology.pdf\")\n",
    "pages = loader.load()\n",
    "pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500,\n",
    "                                            chunk_overlap=200,\n",
    "                                            length_function=len,\n",
    "                                            separators=[\"\\n\\n\", \"\\n\", \" \"])\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=\"text-embedding-ada-002\", openai_api_key=OPENAI_API_KEY\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "embedding_function = get_embedding_function()\n",
    "test_vector = embedding_function.embed_query(\"cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "evaluator = load_evaluator(evaluator=\"embedding_distance\", \n",
    "                            embeddings=embedding_function)\n",
    "\n",
    "evaluator.evaluate_strings(prediction=\"Amsterdam\", reference=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_strings(prediction=\"Paris\", reference=\"Netherlands\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def create_vectorstore(chunks, embedding_function, vectorstore_path):\n",
    "\n",
    "    # Create a list of unique ids for each document based on the content\n",
    "    ids = [str(uuid.uuid5(uuid.NAMESPACE_DNS, doc.page_content)) for doc in chunks]\n",
    "    \n",
    "    # Ensure that only unique docs with unique ids are kept\n",
    "    unique_ids = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    unique_chunks = [] \n",
    "    for chunk, id in zip(chunks, ids):     \n",
    "        if id not in unique_ids:       \n",
    "            unique_ids.add(id)\n",
    "            unique_chunks.append(chunk)\n",
    "        else:\n",
    "            print('Droping duplicate chunk')\n",
    "    # Create a new Chroma database from the documents\n",
    "    vectorstore = Chroma.from_documents(documents=unique_chunks, \n",
    "                                        ids=list(unique_ids),\n",
    "                                        embedding=embedding_function, \n",
    "                                        persist_directory = vectorstore_path)\n",
    "\n",
    "    vectorstore.persist()\n",
    "    \n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorstore\n",
    "vectorstore = create_vectorstore(chunks=chunks, \n",
    "                                 embedding_function=embedding_function, \n",
    "                                 vectorstore_path=\"vectorstore_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Query for relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectorstore\n",
    "vectorstore = Chroma(persist_directory=\"vectorstore_test\", embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever and get relevant chunks\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "relevant_chunks = retriever.invoke(\"What is the title of the paper?\")\n",
    "relevant_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are an assistant for question-answering tasks.\n",
    "Use the following pieces of retrieved context to answer\n",
    "the question. If you don't know the answer, say that you\n",
    "don't know. DON'T MAKE UP ANYTHING.\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate context text\n",
    "context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "\n",
    "# Create prompt\n",
    "prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "prompt = prompt_template.format(context=context_text, \n",
    "                                question=\"What is the title of the paper?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Langchain Expression Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "        )\n",
    "rag_chain.invoke(\"What's the title of this paper?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate structured responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithSources(BaseModel):\n",
    "    \"\"\"An answer to the question, with sources and reasoning.\"\"\"\n",
    "    answer: str = Field(description=\"Answer to question\")\n",
    "    sources: str = Field(description=\"Full direct text chunk from the context used to answer the question\")\n",
    "    reasoning: str = Field(description=\"Explain the reasoning of the answer based on the sources\")\n",
    "    \n",
    "class ExtractedInfo(BaseModel):\n",
    "    \"\"\"Extracted information about the research article\"\"\"\n",
    "    paper_title: AnswerWithSources\n",
    "    paper_summary: AnswerWithSources\n",
    "    publication_year: AnswerWithSources\n",
    "    paper_authors: AnswerWithSources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm.with_structured_output(ExtractedInfo, strict=True)\n",
    "        )\n",
    "\n",
    "rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform response into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_response = rag_chain.invoke(\"Give me the title, summary, publication date, authors of the research paper.\")\n",
    "df = pd.DataFrame([structured_response.dict()])\n",
    "\n",
    "# Transforming into a table with two rows: 'answer' and 'source'\n",
    "answer_row = []\n",
    "source_row = []\n",
    "reasoning_row = []\n",
    "\n",
    "for col in df.columns:\n",
    "    answer_row.append(df[col][0]['answer'])\n",
    "    source_row.append(df[col][0]['sources'])\n",
    "    reasoning_row.append(df[col][0]['reasoning'])\n",
    "\n",
    "# Create new dataframe with two rows: 'answer' and 'source'\n",
    "structured_response_df = pd.DataFrame([answer_row, source_row, reasoning_row], columns=df.columns, index=['answer', 'source', 'reasoning'])\n",
    "structured_response_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
